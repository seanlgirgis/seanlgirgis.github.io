# Detailed CITI Experience: Senior Capacity & Data Engineer (November 2017 – December 2025)

During my 8-year tenure at CITI Financials as a Performance and Capacity Engineer (later evolving into a Senior Capacity & Data Engineer role), I focused on enterprise-wide infrastructure optimization, performance monitoring, and capacity planning for critical banking systems. This involved managing high-scale telemetry data from thousands of endpoints, building automated data pipelines, and developing machine learning models to forecast resource needs. A significant portion of my work centered on Python-based development, where I coded extensively to automate processes, analyze data, and generate actionable insights. Below, I expand on key responsibilities, technical implementations, and achievements, with a deep dive into Python coding and libraries used.

## Capacity Management & Planning
- **Enterprise Capacity Planning and Request Prioritization**: Managed capacity requests across multiple business units, processing telemetry from over 6,000 endpoints (servers, applications, and databases) to ensure optimal resource allocation. I developed Python scripts using **pandas** for data manipulation and **numpy** for numerical computations to analyze utilization trends, forecast demands, and prioritize upgrades. For instance, I built a custom prioritization algorithm that weighted factors like CPU/memory usage, business criticality, and historical peaks, reducing emergency provisioning incidents by approximately 40%.
  
- **Monthly and Seasonal Capacity Reporting**: Generated comprehensive monthly reports on infrastructure utilization, at-risk servers, and forecasting. Using Python with **pandas** to handle large datasets (often millions of rows from CSV/Excel exports), I implemented ETL (Extract, Transform, Load) pipelines to cleanse data—handling missing values, outliers, and inconsistencies via methods like interpolation and z-score filtering. Libraries like **matplotlib** and **seaborn** were used to create visualizations (e.g., heatmaps for utilization patterns and line charts for trends), while **plotly** enabled interactive dashboards for stakeholders. These reports supported budget decisions and prevented service degradation during peak seasons, such as end-of-quarter financial processing.

- **Machine Learning Forecasting Models**: Developed advanced forecasting models to predict capacity requirements 3-6 months in advance. I coded these primarily in Python using **scikit-learn** for regression models (e.g., Random Forest and Gradient Boosting for multivariate predictions) and **Prophet** (from Facebook) for time-series forecasting, incorporating seasonality and holidays specific to banking cycles. Data preprocessing involved **pandas** for feature engineering (e.g., creating lag features and rolling averages) and **statsmodels** for statistical tests like ADF for stationarity. Models were trained on historical data from BMC TrueSight, achieving forecast accuracy improvements of 20-30% over traditional methods, which minimized over-provisioning costs by identifying underutilized resources.

- **Seasonal Risk Analysis**: Conducted in-depth analysis to flag servers at risk during high-load periods. I wrote Python scripts integrating **scipy** for statistical modeling (e.g., percentile calculations for P95 metrics) and **joblib** for parallel processing to handle large-scale simulations, reducing analysis time from days to hours.

## Monitoring & Performance Tools Administration
- **Data Integration and Pipeline Automation**: Built automated data pipelines to ingest, transform, and load performance and capacity metrics by directly querying the Oracle backup database (replica) instead of relying on live API calls. Using Python with **sqlalchemy** (for ORM and connection pooling) and **pandas** (for efficient data handling), I extracted high-volume telemetry data (P95 metrics, utilization trends, and event logs) from the backup schema. This approach provided more reliable, historical-depth access while avoiding load on production monitoring systems. I coded complex SQL queries (including window functions, pivots, and joins across multiple tables) embedded via **sqlalchemy** text() constructs, then used **pandas** to merge, correlate, and enrich datasets from disparate sources — such as linking application-level bottlenecks (from CA Wily/AppDynamics extracts) to underlying infrastructure capacity constraints (BMC TrueSight/TSCO data). Custom scripts automated threshold configurations, alert rule generation, and anomaly detection logic, enabling true end-to-end visibility across the monitoring stack. This database-driven integration method improved data freshness for reporting, reduced dependency on external API availability, and supported more robust historical trend analysis.

## Data Management, Optimization, & Reporting
- **Data Cleansing and Quality Assurance**: Performed extensive data cleansing on monitoring feeds using Python scripts. For example, I used **pandas** DataFrames to detect and impute anomalies (e.g., via KNN imputation from **scikit-learn**), ensuring reliable inputs for ML models. I also integrated **numpy** for vectorized operations on time-series data, speeding up processing by 5x.

- **Interactive Dashboards and Visualizations**: Developed executive dashboards using Python libraries like **plotly** for interactive charts (e.g., drill-down utilization maps) and **dash** (Plotly's web framework) for deployment. These provided real-time insights into resource trends, forecasts, and optimization opportunities, often exported to PDF/Excel via **reportlab** or **openpyxl**.

- **Optimization and Cost Management**: Applied ML techniques in Python to identify underutilized patterns, using **scikit-learn's clustering** (e.g., K-Means) on performance metrics to recommend consolidations. Collaborated with teams to right-size resources based on these insights, resulting in cost savings through reduced hardware needs while maintaining SLAs.

- **Scripting and Automation Enhancements**: Beyond Python, I maintained legacy Perl/R scripts but migrated many to Python for better maintainability. For data mining, I used **BeautifulSoup** for scraping internal reports when needed, and **multiprocessing** to parallelize tasks on multi-core servers.

This role honed my Python expertise across data engineering, ML, and automation, handling enterprise-scale challenges in a regulated financial environment. I coded thousands of lines in Python, leveraging libraries like pandas, numpy, scikit-learn, Prophet, matplotlib, seaborn, plotly, sqlalchemy, scipy, statsmodels, requests, logging, joblib, and dash. These efforts directly contributed to proactive infrastructure management, cost efficiencies, and performance improvements across CITI's global operations. If needed, I can provide code samples or further details on specific projects.